https://www.youtube.com/@HungyiLeeNTU 2021机器学习



# 模型思路

![image-20250411173204836](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250411173204836.png)

调整思路：

```
训练集上损失大：

	1、模型复杂度不够

	2、优化方法有问题
	
	怎么分辨：使用简单的模型，简单模型一般不会存在优化不到位的问题，如果简单模型的损失更小，说明现在模型的优化方法有问题

	解决方法：

		1、使用更复杂的模型
		
		2、略

训练集损失小，测试集损失大：
		
	1、过拟合
		
	2、测试集出现了训练集没有的数据，分布不同
	
	怎么分辨：根据业务理解，是不是分布不同。
	
	解决方法：
		
		1、更多的训练数据、简化模型等。
```

建模思路：

```
矛盾问题：模型简单了会出现 model bias ，复杂了又会overfitting。怎么建模避免这个矛盾

建模过程：使用多个模型，每个模型都进行多则交叉验证（将训练集分为训练集和验证集），选择评价效果最好的那个模型。
		注意，不从模型预测得到的测试集结果调整模型。
```



# optimization

求目标函数最小值的优化方法，如梯度下降



train loss 不再下降原因：gradient 在 0 处收敛，即达到 critical point。

​	一、 局部最小值 (local minima)，无路可走。

​	二、鞍点 (saddle point)，仍有路可走，使loss降低。



区分 local minima 和 saddle point，了解即可：

![image-20250825192933065](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250825192933065.png)



实际上，维度越高，local minima 会变成 saddle point ，local minima 越少。所以卡在 saddle point 的情况是大部分。



# batch

1 **epoch** = see all the batches once，然后 Shuffle after each epoch



batch size 需要选取合适的数值：

​	一、batch size 越大，训练速度一般越快

​		*因为有GPU的并行运算，batch size 在一定大小范围内的训练时间是相同的，batch size 越大，update 次数越少，总时间越短。*

​	二、batch size 越小，train 的效果越好

​		*batch size 大，可能在某一次 update 就达到了 local minima。batch size 小，update 次数多，更不容易陷入到 local minima。*

​	三、batch size 越小，test 效果越好

​		*batch size 大，local minima 更容易狭窄，反之，batch size 小，会更容易达到比较宽阔的local minima。*

​		*因为 train 和 test 的参数本身就有差距，所以当 local minima 比较宽阔的时候，更不容易过拟合*



# momentum



# Learning Rate

不同的参数，不同的步数，需要不同的学习率：

一、使用学习率除以 σ 作为新的学习率，σ在 error surface 平缓的地方小，在陡峭的地方大。

![image-20250902182107864](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902182107864.png)

二、加入一个 α 项，可以调整学习率变化倾向于 最近或者之前 的梯度。

![image-20250902183946287](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902183946287.png)

三、Adam

![image-20250902184454227](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902184454227.png)

四、RAdam，根据不同的步数，调整学习率的两种方法。

![image-20250902185538235](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902185538235.png)



### 梯度下降的提升方式

![image-20250902190258779](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902190258779.png)



# Batch Normalization



**为什么要批次标准化**

当 x1，x2 范围差距过大，为了得到最佳的loss，w1，w2的差距也会很大。会得到一个很难达到 local minima 的 error surface。

所以，为了得到一个比较好的 error surface，需要对 x1，x2 进行 Normalization。

进行批次标准化的原因就是得到一个比较好的  error surface。



**批次标准化过程**

![image-20250908174828261](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250908174828261.png)

一、选定一个批次，比如64个样本，5个特征的数据，对每个特征进行标准化（减均值，除方差）。

二、输入第一层神经网络，假设有3个神经元，那么每个样本就会从5个数变成3个数，64个样本都出来后，还可以进行再标准化。

三、为了避免标准化后均值为0的强制限制，可以加上 β 和 γ 两个随机超参数。

四、再进入激活函数，得到正常的输出结果。在下一层神经网络重复这个过程。



**tips**

一、还有其他的标准化方法。

二、其他使得 error surface 变得更平滑的方法，也能达到批次标准化的效果。



# CNN

卷积神经网络 (Convolutional Neural Networks）

常用于图片识别，找到图片中的 部分特征 pattern，进而识别整张图片。



### 卷积层介绍

Convolutional layer



输入：一张图片，拥有长、宽、channel （图片的channel 就是RGB三色），变成矩阵可能是 100 * 100 * 3。

**kernel**  ：就是一个小范围的矩阵，用 kernel 和图片上相同范围矩阵做运算（卷积），就可以提取图片的部分特征 pattern。

​					kernel size 假设为 3 * 3， 一般包含所有channel 。

![image-20250909183738259](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250909183738259.png)

kernel 对图片 kernel size 的范围进行一次卷积运算，等价于一个神经元对图片 kernel size 的范围进行计算。

kernel 需要在图片上 滑动，才能得到图片上的所有特征，这个**步距**叫做 **stide**。

每次 kernel 滑动，也会有一个新的神经元看到。这些神经元共享参数，也就是kernel，叫做 **parmeter sharing**。

一个 kernel size 的范围通常被多个神经元看到，每个神经元都是不同的 kernel 。

kernel 移动到图片边缘时，图片的像素小于 kernel size，需要对图片缺失像素填充，这个操作叫做 **padding**。



同一个 kernel 在整张图片运行一次后，会根据卷积操作得到一个新的二维矩阵。多个 kernel 会得到最终的输出  **feature map**。

feature map 长宽由 卷积运算和池化操作 决定，channel 有 kernel 的个数决定。



tips：

**receptive field** ，一个神经元看到的原图片的范围，可以是图片的任意长宽channel 。 深层神经元看见的receptive field范围很大。

**filter**，CNN 中基本等于kernel ，非要区别， kernel 也可以认为是不包含 channel 的，而 filter 包含channel ，一个filter 有多个kernel 。



### 卷积神经网络 

Convolutional Neural Networks



1. 卷积层

2. 池化操作 pooling

```
max pooling ，从自定义范围的每组像素中选最大的数作为代表，channel不变。
pooling 一般在多次 Convolutional layer后操作。
减少运算量。
```

3. flatten： 矩阵变成一个向量

4. fully connected network：softmax后得到分类结果



# Self-attention

自注意力机制



### 引入



#### Sequence

之前的神经网络的输入都是 **vector**，也就是固定的一维向量。

接下来要学习神经网络的更复杂输入，也就是 **Sequence**，a set of vectors。



什么是 Sequence：

一、文字处理中，一句话可以变成 Sequence，每一个词通过 one-hot 或者 word embedding 的方式变成一个向量。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-39fdaa2084a867e0cb68a1b9812ce93d_r.jpg)

二、声音序列中，一段声音也可以变成 Sequence，把一段窗口内的语音片段（frame）变成一个向量，1秒中的语音大概有 100 个 frame。

三、图，一张图也可以变成 Sequence，每一个节点 node 可以变成一个向量。



#### 神经网络的输入输出类型

一、输入是 Vector，输出也是 回归或者分类结果，也就是一个lable。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-b699ead5caf912a88ab3904dbb3b7a69_1440w.jpg)

二、当输入是 Sequence时，输出可以是

```
1、m to m ，Sequence 中的每一个向量对应一个label。
2、m to 1，一个 Sequence 输出一个 label。
3、seq to seq，输入是 Sequence，模型决定输出几个label。
```

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-c89a51b30492d1832bc42da63ce7633e_r.jpg)

其中，第三种情况叫做 **seq2seq**。

第一种情况叫做 **sequence labeling**，是 Self-attention 解决的问题。



### self-attention 的思想

以 词性标注 任务为例，输入是一组单词，输出是对应每个单词的词性，是典型的 sequence labeling问题。

在 词性标注 任务中，输出不仅受单词本身的影响，还与这个单词在整个句子中的位置或者说其他单词有关系。



self-attention 的主要思路就是，计算出输入向量之间的 **关联度 α**，再乘以每个向量本身的权重，最后相加即为这个向量对应的输出结果。

在 词性标注 任务中，就是先计算输入单词和所有单词之间的关联度，再乘以其他单词本身的数值，最后相加即为这个单词的词性。



通用的说，self-attention就是在计算输入对应的输出时，不仅考虑了输入的影响，还考虑了整个输入sequence 的影响。



### self-attention 的结构



#### 注意力权重 α 的计算

输入向量之间的 **关联度 α** 的计算方法，有很多方法，比如 Dot-product 方法和 Additive 方法。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-9fa9e2ea7b87d3623227ee45202086fe_r.jpg)

我们主要学习 Dot-product 方法，这是最常用的方法，也是 Transformer 中用的方法。



Dot-product 方法 计算 注意力权重 α ：以计算 a1向量和其他向量之间的 α 为例。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-39b7e423ba3bfe911b907c4765768b41_r.jpg)

每个输入向量都生成 **query** 向量 和 **key** 向量，query 向量用 Wq 矩阵生成，key 向量用 Wk 矩阵生成，见上图底部。

注意，每个向量的  Wq 矩阵 和  Wk 矩阵都是相同的，代表每个向量都用同样的方法去计算关系。



a1向量和其他向量之间关联度计算：就是用a1向量的 query **点积** 所有向量的 key ，即可得到 α。

最后 使用 Soft-max 函数 或者其他激活函数 做 Normalization，即得到最后的 attention score 或者说 α。

 

#### attention 层后的计算

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-f28102dcc1812a8784981a0da494dad8_r.jpg)

引入向量 value，代表每个输入向量本身的值，value向量用 Wv 矩阵生成。见上图底部。

使用a1所有的attention score 和 V  加权求和，计算出a1对应输出向量b1。



最终，谁和当前词的关系越大，其信息就越会被抽出来。



### 并行计算

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-469693031583f2a7b9321a640ca8f444_1440w.jpg)

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-d70be2b5a56ab773f798d5da2615713e_r.jpg)

A 就是 attention score 的矩阵。

Self-attention 就是训练得出 Wq, Wk, Wv 这三个矩阵。



### Multi-head Attention

多头注意力 机制



当 输入向量间需要多种联系，才能得到结果，计算多个 注意力权重 时，引入多头注意力 机制。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-fb5641b01ecfbe5c540c60a5ba10e6e8_1440w.jpg)

首先，每个输入向量生成多个 q，k，v。

每组q，k，v，都利用 Self-attention 的计算过程， 生成 attention score，生成输出向量b。

最后，增加一个Wo矩阵，和所有组生成b相乘，得到最终的输出。



### Position Encoding

Self-attention 的计算过程时并行的，每个输入向量，得到对应的输出向量，输入向量相互间的前后位置并没有被考虑到。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-1296e6c429d16b16ffdad0c70083d868_r.jpg)

**位置编码（Position Encoding）**：加入位置向量e，e+a得到新的输入向量x，包括语义信息和位置信息， 用x当做输入向量，进行Self-attention 的计算

位置编码的方法有很多种，上图所示是一种，每一列都是一个向量e。



为什么是“相加”而不是“拼接”？

1. 维度一致：拼接会导致维度变多。
2. 足够有效：权重矩阵w可以灵活的选择和使用 融合后的不同信息。



### 与CNN比较

CNN 多用于图像识别，Self-attention 用于图像识别，就是把每一个 channel 当作一个vector，一张图片就是一组 vector。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-c611abaab97d1117a85ce367c4cb38e9_1440w.jpg)

具体的 Self-attention 和 CNN 的转化过程先不学，只需要记住 CNN 是 Self-attention的特例。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-109abf697200bd20392d165e8781094c_r.jpg)



tips：当数据量小的时候，CNN 的表现更好，当数据量非常大时，self-attention 的效果（才）会更好。



# Transformer

**Transformer** 是 **seq2seq** 的model。输入和输出的长度不一定相等。应用比如：机器翻译，语音翻译



Transformer 的结构如下图。分为 **encoder** 和 **decoder** 两部分。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-55f204266d867902bf9f64daae1b98d2_1440w.jpg)

encoder：读懂输入的内容，并把它变成一种机器更容易处理的“深度理解版”表示。

decoder：根据 Encoder 提供的“深度理解版”信息，和已经生成的内容，一步步生成你想要的结果。就是输出一个向量后，作为下一个输入向量，这样逐步生成。



举例：

- **摘要**：Encoder 读长文章，Decoder 写简短摘要。
- **问答**：Encoder 读问题和文章，Decoder 生成答案。
- **聊天机器人**：Encoder 读你的问题，Decoder 生成回复。
- **甚至生成图片、代码**：原理类似，只是输入和输出的形式不同。



### encoder

transformer的一个encoder block的输入输出示例如下：

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-d8815647a6b34a0f5596d1948fcba699_1440w.jpg)



### decoder

decoder 有两个需要注意的点，一个是 **Masked Multi-Head Attention**；一个是 encoder 和 decoder 的连接部分，**Cross attention**



#### Masked Self-attention

我们知道，decoder 的输入包括两个部分，一个是encoder 的输出，一个是 decoder 本身的输出 右移一位作为输入。

由此可知，推理时，decoder 的输入是逐渐出现的，输出一次才能得到下一次输入。训练时为了和推理保持一致，也不能同时看到所有的decoder输入向量。



所以不能使用 Self-attention，而是使用 Masked Self-attention，具体的是 Masked Multi-Head Attention。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-06a599c6b0f300230f5debfd06fde4fb_r.jpg)

产生b1只能用到a1。

产生b2只能用到a1，a2。（上图所示）

产生b3只能用到a1，a2，a3。

产生b4只能用到a1，a2，a3，a4。

也就是只能看左边的，相当于预测的时候只能看前面的单词，只能用前面的单词拿来预测。输出的东西是一个一个产生的，所以只能考虑左边的



#### Cross attention

Cross attention 是 encoder 和 decoder 的连接部分。



用 encoder 的 k 和 v 去与 decoder 的 q 去做attention，就是Cross attention。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-779e2e77fa70052dbcc8f132ce1670c2_r.jpg)



tips：原论文是encoder的最后一层与decoder相连。但是还有其他的很多连接。



#### decoder 输出长度

增加一个 end 符号，当输出 end符号时，就结束输出。



#### Autoregression

自回归



这种 输入由输出决定 的方式叫做自回归。

自回归的方式分为两种，AT 和NAT。

![image-20250916171545879](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250916171545879.png)



### 训练推理



#### train

输入音频，输出“机器学习”。

每一个输出向量就是一张 汉字概率分布表，真实的分布表和预测出的分布表使用 交叉熵损失 作为代价函数。

训练 就是求所有输出向量总的 交叉熵损失最小 时的参数。

![img](https://raw.githubusercontent.com/zhanghongyang42/images/main/v2-1890b43202ad988006ad5419cafb13d7_1440w.jpg)



#### inference

使用 自回归（Autoregression）的方式进行推理。















































