# 神经网络

起源：模仿大脑的算法。

发展：1950开始有，1980末90初被使用，1990末不再使用，2005再次发展。

领域：语音-->图像-->文本（NLP）--> ......



### 模型定义

神经元（unit）：可以理解为一个简单的模型，可以有一个或几个输入，得到一个输出。

​								具体的，神经元一般由 线性计算+激活函数 组成。

参数（weight）：神经元中可以变化，需要训练的参数。



模型的各个层都是一组神经元组成的。神经网络的层数时不包括输入层。

输入层（Input Layer）：输入层只是一组输入数字，直接传给下一层。可以理解为特征。

隐藏层（Hidden Layers）：隐藏层是一组神经元。每个神经元的输出 可以理解为 特征中隐藏的信息 或者 特征中间预测结果。

输出层（Output Layer）：最后一组作为输出的神经元。



神经网络架构：多少个隐藏层和输出层，每层多少个神经元（unit）。

很多情况下，随着层数的增加，神经元减少的结构比较有效。

![image-20220818113346172](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20220818113346172.png)

a代表每层输出向量，上标是层数，下标是神经元位置，向量w和数字b是参数。



forward propagation 正向传播：已知参数，神经网络每一层从前往后计算。



### TensorFlow-训练

Tensor（张量）是 TensorFlow 的输入数据的类型，可以理解为矩阵。可以和numpy互相转化。

使用向量化计算，使得w·x的矩阵乘法更快速。

深度学习中，分为 **训练** 和 **推理** 两部分。



![image-20220818201333525](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20220818201333525.png)

1.写出模型。2.编译确定损失函数。3.进行训练。



Dense Layer：该层神经网络中的每个神经元都会接受所有的输入。



### 激活函数



线性激活函数 ：g(z) = z

ReLU：g(z) = max(0,z)

Sigmoid：$g(x) = \frac{1}{1 + e^{-x}}$



输出层激活函数的选择：

二分类 问题，输出层 激活函数通常使用 sigmoid。输出两个概率。

多分类 问题，输出层 激活函数通常使用 softmax。输出多个概率。

回归 问题，输出层 激活函数通常使用 线性激活函数。因为输出有正有负

回归问题，y只有正值，输出层 激活函数通常使用ReLU。输出永远非负



隐藏层 激活函数 通常使用 ReLU，一般绝对不会使用线性激活函数（会导致神经网络及其简单至失效）。



ReLU 比 sigmoid 更经常使用的原因：

1. ReLU计算复杂性低，速度更快。
2. 在使用梯度下降求损失函数最小时，函数平滑的地方下降速度缓慢，sigmoid两侧都比较平滑，不容易计算。



为什么要使用激活函数：如果不使用激活函数，神经网络将变成线性回归函数，不再发挥作用。



# 神经网络的多分类

神经网络的多分类 通过 Softmax 函数实现，通过学习 Softmax regression 算法学习  Softmax 函数。



## Softmax regression

Softmax regression 是一种多分类算法.



### 模型表示


$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}, \quad \text{for } i = 1, 2, \dots, K
$$

其中  $z = Wx+b$



y 有 K 个分类，Softmax regression 就有 K 组 wb参数 。K 个分类的概率相加为 1。



分解写法详见下图右，合并写法详见下图左下。

![image-20230323162346585](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20230323162346585.png)



### 代价函数

![image-20230323171334033](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20230323171334033.png)

softmax 代价函数是对每组参数分别计算代价的和
$$
J(W, b) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{n} [y_i^j \log(p_i^j)]
$$

- m：样本数量；
- n：类别数量；
- $y_i^j$：第i个样本的真实标签向量的第j个元素，取值为0或1；
- $p_i^j$：第i个样本的预测概率向量的第j个元素，由Softmax函数计算得出；



## 多标签分类

神经网络的多分类（multi-label classfication）。 将Softmax regression 作为 神经网络的输出层即可实现。



Softmax 激活函数是特殊的，用到这一层所有神经元的线性计算结果。

![image-20250409145022046](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250409145022046.png)



## 多输出分类

与多标签分类相比，只是输出层的激活函数不同，即可得到多输出分类。

![image-20230323185523772](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20220819182200545.png)



# 优化算法

### 梯度下降

略



### Adam

优化的梯度下降算法：有多少个Wb参数，就有多少个步长，并且每个步长可以根据下降的结果自动调整。



# 层类型

Dense Layer：每一个神经元接受所有输入



Convolutional Layer：每一个神经元接受部分输入

训练的更快，避免过拟合。





# 神经网络求解





训练神经网络：

1. 参数的随机初始化
2. 利用正向传播方法计算所有的
3. 编写计算代价函数  的代码
4. 利用反向传播方法计算所有偏导数
5. 利用数值检验方法检验这些偏导数
6. 使用优化算法来最小化代价函数



### 代价函数

逻辑回归的代价函数是二分类的交叉熵损失函数，此处神经网络的代价函数是多分类的交叉熵损失函数。

![image-20230103145638282](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20230103145638282.png)



### 反向传播

如不理解，可以继续看视频。

反向传播算法用来计算代价函数的偏导数。



前向传播算法

![image-20230103150517721](picture\image-20230103150517721.png)

反向传播算法

![image-20230103150801595](picture\image-20230103150801595.png)



### 梯度检验

略



### 随机初始化

神经网络的参数初始化为随机值。

https://zhuanlan.zhihu.com/p/41465442



# 神经网络优化

使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小

使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。

通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。



对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，

可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层 层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。





# 迁移学习

迁移学习是指导思想，预训练和微调是技术手段。

比如在1亿张图片上训练识别猫狗等，把得到的参数在 100张图片上训练手写数字识别，这个过程就叫做迁移学习。可能需要保持输入数据类型一致。



有监督的预训练：在大规模数据集上学习通用特征

微调包括两种方法：

​	1、只改变输出层的参数，其他用预训练参数。

​	2、在预训练参数的基础上，使用较小的学习率重新训练所有参数



![image-20250409172149498](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250409172149498.png)



